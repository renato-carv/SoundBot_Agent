import json
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
from src.config.settings import settings
from src.utils.logger import logger  # ‚úÖ Importando o logger configurado

class AIService:
    def __init__(self):
        self.llm = ChatGroq(
            model="llama-3.1-8b-instant",
            temperature=0.7,
            groq_api_key=settings.GROQ_API_KEY
        )
        logger.info("AIService initialized with model llama-3.1-8b-instant")

    def interpret_request(self, text: str) -> dict:
        system_prompt = """
            Voc√™ √© um assistente musical que interpreta pedidos dos usu√°rios e converte em JSON estruturado.

            Retorne SOMENTE o JSON, no formato:
            {
            "acao": "recomendar" | "buscar_artista" | "buscar_musica" | "desconhecido",
            "humor": "feliz" | "triste" | "animado" | "calmo" | "rom√¢ntico" | "energ√©tico" | "nost√°lgico" | "festa" | "focado" | null,
            "artista": string | null,
            "musica": string | null
            }
        """

        messages = [SystemMessage(content=system_prompt), HumanMessage(content=text)]
        logger.info(f"Interpreting user request: '{text}'")

        try:
            response = self.llm.invoke(messages)
            content = response.content.strip()

            logger.debug(f"Raw LLM response: {content}")

            # Remove poss√≠veis delimitadores de bloco de c√≥digo Markdown
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()

            parsed = json.loads(content)
            logger.info(f"Successfully interpreted request: {parsed}")
            return parsed

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing failed: {e} | Response content: {content}")
        except Exception as e:
            logger.exception(f"Error interpreting request: {e}")

        # Fallback padr√£o em caso de erro
        return {
            "acao": "desconhecido",
            "humor": None,
            "artista": None,
            "musica": None,
        }

    def generate_reply(
        self,
        context: str,
        recommendations: list,
        mood: str = None,
        artist: str = None,
        song: str = None
    ) -> str:
        system_prompt = """
        Voc√™ √© um assistente musical amig√°vel e natural.
        Retorne **apenas** um JSON v√°lido no formato:
        {
        "reply": "Mensagem simp√°ticas e humanas, sem citar os nomes das m√∫sicas. De hip√≥tese nenhuma, seja grosseiro.",
        "mood": "<humor_detectado ou null>",
        "recommendations": ["m√∫sica - artista", ...]
        }

        Regras importantes:
        - O campo "reply" deve conter uma frase natural (ex: "Acho que essas m√∫sicas combinam com seu humor! üéß")
        - NUNCA inclua os nomes das m√∫sicas no "reply"
        - O campo "recommendations" deve conter exatamente as m√∫sicas listadas
        - O texto deve estar em portugu√™s e soar humano
        - N√ÉO coloque markdown nem blocos de c√≥digo
        """

        user_context = f"""
        Contexto: {context}
        Humor detectado: {mood}
        Artista: {artist}
        M√∫sica: {song}
        Recomenda√ß√µes: {recommendations if recommendations else 'nenhuma encontrada'}
        """

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_context)
        ]

        try:
            response = self.llm.invoke(messages)
            content = response.content.strip()

            # remove ```json e outros delimitadores
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()

            data = json.loads(content)

            # garante que reply sempre exista
            reply = data.get("reply", "Espero que goste dessas m√∫sicas! üé∂")
            mood = data.get("mood", mood)
            recommendations = data.get("recommendations", recommendations)

            logger.info(f"AI formatted reply: {data}")
            return reply  # s√≥ retornamos o texto pro ChatController

        except json.JSONDecodeError:
            logger.warning(f"LLM did not return JSON. Raw: {response.content}")
            return "Aqui est√£o algumas m√∫sicas que acho que voc√™ vai gostar! üéß"
        except Exception as e:
            logger.error(f"Error generating reply: {e}", exc_info=True)
            return "Tive um probleminha para formular a resposta agora, mas posso tentar de novo se quiser!"
